{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_dim = 32\n",
    "max_digits = 6 # maximum number of digits in the two numbers we are adding/mutliplying/dividing\n",
    "context_length = 2*max_digits + 2*max_digits + 2 # number1 * number2 = number3 (which can have 2*max_digits)\n",
    "batch_size = 32\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.masked = masked\n",
    "        self.q = nn.Linear(emb_dim, emb_dim, bias=False) #compute for all heads in parrallel, n_heads * head_size = emb_dim\n",
    "        self.k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        if self.masked:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        Q = Q.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #self attention needs to be done individually for each head\n",
    "        K = K.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #transpose needed so we do self attention in the given char context not between the different heads\n",
    "        V = V.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2)\n",
    "        wei = Q @ K.transpose(-1, -2)\n",
    "        if self.masked:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = wei / C**-0.5\n",
    "        out = F.softmax(wei, dim=-1) @ V\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "        nn.Linear(emb_dim, 4*emb_dim), nn.ReLU(),\n",
    "        nn.Linear(4*emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_head)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        out = x + self.ff(self.ln2(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads=4, n_blocks=2): \n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, emb_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_heads) for _ in range(n_blocks)])\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embs = self.token_embedding(idx) # B, T -> B, T, C\n",
    "        pos_embs = self.position_embedding(torch.arange(T))\n",
    "        out = token_embs + pos_embs\n",
    "        out = self.blocks(out)\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def calculate(self, idx, max_tokens=100):\n",
    "        for _ in range(max_tokens): # just to make sure we dont run into infinite loop if model fails to end its output\n",
    "            context = idx[:, -context_length:]\n",
    "            logits, _ = self(context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probs, keepdim=True) # use argmax instead of multinomial, there is only one correct answer\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "            if next_token == 15:\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {'+': 10, '-':11, '=': 14, '<END>':15}\n",
    "itos= {i: op for op, i in stoi.items()}\n",
    "\n",
    "def encode(num1, num2, op, res=None):\n",
    "    # encode symbols\n",
    "    op_enc = torch.tensor(stoi[op], dtype=torch.long).view(1)\n",
    "    equals_enc = torch.tensor(stoi['='], dtype=torch.long).view(1)\n",
    "\n",
    "    # encode input numbers (left pad with zeros until num_digits)\n",
    "    def _encode_num(num, num_digits, reverse=False):\n",
    "        if reverse:\n",
    "            out = torch.tensor([int(digit) for digit in reversed(str(num).zfill(num_digits))], dtype=torch.long)\n",
    "        else:\n",
    "            out = torch.tensor([int(digit) for digit in str(num).zfill(num_digits)], dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "    num1_enc = _encode_num(num1, max_digits)\n",
    "    num2_enc = _encode_num(num2, max_digits)\n",
    "\n",
    "    if res == None:\n",
    "        out = torch.cat([num1_enc, op_enc, num2_enc, equals_enc])\n",
    "    else:\n",
    "        res_enc = _encode_num(res, 2*max_digits, True)\n",
    "        out = torch.cat([num1_enc, op_enc, num2_enc, equals_enc, res_enc])\n",
    "\n",
    "    return out\n",
    "\n",
    "def decode(x):\n",
    "    out = []\n",
    "    for idx in x:\n",
    "        if idx < 10: out.append(str(idx.item())) # if its a digit just add it as a str\n",
    "        elif idx == stoi['<END>']: break # END token means we are done\n",
    "        else: out.append(itos[idx.item()]) # otherwise encode op\n",
    "\n",
    "    out =  \"\".join(out)\n",
    "    out = out[:-2*max_digits] + out[:(-2*max_digits)-1:-1] # reverse the result\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  4,  1,  2,  6,  5, 11,  2,  1,  8,  6,  4,  3, 14,  2,  2,  6,  2,\n",
       "          2,  2,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100,    2,    2,    6,    2,    2,    2,    0,    0,    0,    0,    0,\n",
       "            0,   15]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we sample two random numbers as input and their sum as the label\n",
    "def sample_mathproblems(num_problems): \n",
    "    ops = torch.randint(10, 12, (num_problems, ), dtype=torch.long)\n",
    "    all_nums = torch.randint(0, 10**max_digits, (num_problems, 2), dtype=torch.long)\n",
    "    \n",
    "    x = torch.zeros((num_problems, context_length), dtype=torch.long)\n",
    "\n",
    "    for i, (nums, op) in enumerate(zip(all_nums, ops)):\n",
    "        num1, num2 = nums[0].item(), nums[1].item()\n",
    "        op_c = itos[op.item()]\n",
    "        match op_c:\n",
    "            case '+':\n",
    "                res = num1 + num2\n",
    "            case '-':\n",
    "                if num2 > num1: num1, num2 = num2, num1 #swap because we dont want negative numbers\n",
    "                res = num1 - num2\n",
    "        x[i] = encode(num1, num2, op_c, res)\n",
    "\n",
    "    input_size = 2*max_digits+2\n",
    "    masked_loss = -100 * torch.ones((num_problems, input_size-1), dtype=torch.long)\n",
    "    end_token = stoi['<END>'] * torch.ones((num_problems, 1), dtype=torch.long)\n",
    "    y = torch.cat([masked_loss, x[:, input_size:], end_token], dim=1)\n",
    "    return x, y\n",
    "\n",
    "n_samples = 1_000_000\n",
    "x, y = sample_mathproblems(n_samples)\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800000, 26]),\n",
       " torch.Size([800000, 26]),\n",
       " torch.Size([100000, 26]),\n",
       " torch.Size([100000, 26]),\n",
       " torch.Size([100000, 26]),\n",
       " torch.Size([100000, 26]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * n_samples)\n",
    "val_size = int(0.9 * n_samples)\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_val, y_val = x[train_size:val_size], y[train_size:val_size]\n",
    "x_test, y_test = x[val_size:], y[val_size:]\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            if split == 'train':\n",
    "                idx = torch.randint(0, len(x_train), (batch_size, ))\n",
    "                X, Y = x_train[idx], y_train[idx]\n",
    "            elif split == 'val':\n",
    "                idx = torch.randint(0, len(x_val), (batch_size, ))\n",
    "                X, Y = x_val[idx], y_val[idx]\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_gpt(model, optimizer, train_steps=100_000, eval_iters=200):\n",
    "    for step in range(train_steps):\n",
    "        # forward pass\n",
    "        idx = torch.randint(0, len(x_train), (batch_size,))\n",
    "        x, y = x_train[idx], y_train[idx]\n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10_000 == 0:\n",
    "            losses = estimate_loss(model, eval_iters) \n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"{step}/{train_steps} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100000 - Train Loss: 2.7199, Val Loss: 2.7197\n",
      "10000/100000 - Train Loss: 0.3773, Val Loss: 0.3759\n",
      "20000/100000 - Train Loss: 0.3599, Val Loss: 0.3599\n",
      "30000/100000 - Train Loss: 0.3552, Val Loss: 0.3552\n",
      "40000/100000 - Train Loss: 0.3552, Val Loss: 0.3552\n",
      "50000/100000 - Train Loss: 0.1586, Val Loss: 0.1585\n",
      "60000/100000 - Train Loss: 0.0021, Val Loss: 0.0024\n",
      "70000/100000 - Train Loss: 0.0002, Val Loss: 0.0002\n",
      "80000/100000 - Train Loss: 0.0008, Val Loss: 0.0008\n",
      "90000/100000 - Train Loss: 0.0001, Val Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "model = GPT(16)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_gpt(model, optimizer, train_steps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: 000007+000003=000000000010, Correct Output: 10\n",
      "Model Output: 012358+001987=000000014345, Correct Output: 14345\n",
      "Model Output: 000014-000009=000000000005, Correct Output: 5\n",
      "Model Output: 112343-007863=000000104480, Correct Output: 104480\n"
     ]
    }
   ],
   "source": [
    "sa1, sa2, res_sa = 7, 3, 7+3\n",
    "simple_add = encode(sa1, sa2, '+').view(1, -1)\n",
    "print(f\"Model Output: {decode(model.calculate(simple_add)[0])}, Correct Output: {res_sa}\")\n",
    "\n",
    "ca1, ca2, res_ca = 12358, 1987, 12358+1987\n",
    "complex_add = encode(ca1, ca2, '+').view(1, -1)\n",
    "print(f\"Model Output: {decode(model.calculate(complex_add)[0])}, Correct Output: {res_ca}\")\n",
    "\n",
    "ss1, ss2, res_ss = 14, 9, 14-9\n",
    "simple_sub = encode(ss1, ss2, '-').view(1, -1)\n",
    "print(f\"Model Output: {decode(model.calculate(simple_sub)[0])}, Correct Output: {res_ss}\")\n",
    "\n",
    "cs1, cs2, res_cs = 112342, 7863, 112343-7863\n",
    "complex_sub = encode(112343, 7863, '-').view(1, -1)\n",
    "print(f\"Model Output: {decode(model.calculate(complex_sub)[0])}, Correct Output: {res_cs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
