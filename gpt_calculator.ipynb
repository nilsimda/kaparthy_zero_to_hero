{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_dim = 32\n",
    "max_digits = 6 # maximum number of digits in the two numbers we are adding\n",
    "context_length = 2*max_digits + max_digits+1 + 2 # number1 + number2 = number3 (which can have max_digits+1)\n",
    "batch_size = 32\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.masked = masked\n",
    "        self.q = nn.Linear(emb_dim, emb_dim, bias=False) #compute for all heads in parrallel, n_heads * head_size = emb_dim\n",
    "        self.k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        if self.masked:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        Q = Q.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #self attention needs to be done individually for each head\n",
    "        K = K.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #transpose needed so we do self attention in the given char context not between the different heads\n",
    "        V = V.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2)\n",
    "        wei = Q @ K.transpose(-1, -2)\n",
    "        if self.masked:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = wei / C**-0.5\n",
    "        out = F.softmax(wei, dim=-1) @ V\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "        nn.Linear(emb_dim, 4*emb_dim), nn.ReLU(),\n",
    "        nn.Linear(4*emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_head)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        out = x + self.ff(self.ln2(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads=4, n_blocks=2): \n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, emb_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_heads) for _ in range(n_blocks)])\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embs = self.token_embedding(idx) # B, T -> B, T, C\n",
    "        pos_embs = self.position_embedding(torch.arange(T))\n",
    "        out = token_embs + pos_embs\n",
    "        out = self.blocks(out)\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def calculate(self, idx, max_tokens=100):\n",
    "        for _ in range(max_tokens): # just to make sure we dont run into infinite loop if model fails to end its output\n",
    "            context = idx[:, -context_length:]\n",
    "            logits, _ = self(context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probs, keepdim=True) # use argmax instead of multinomial, there is only one correct answer\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "            if next_token == 15:\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 21]) torch.Size([1000000, 21])\n"
     ]
    }
   ],
   "source": [
    "optoi = {'+': 10, '-':11, '*':12, '/': 13, '=': 14, '<END>':15}\n",
    "itoop = {i: op for op, i in optoi.items()}\n",
    "\n",
    "# we sample two random numbers as input and their sum as the label\n",
    "def sample_mathproblems(num_problems): \n",
    "    operation = '+' #TODO: Expand to all four basic operations\n",
    "\n",
    "    # sample input data\n",
    "    first_nums = torch.randint(0, 9, (num_problems, max_digits), dtype=torch.long)\n",
    "    sum_symbols = torch.ones((num_problems,1), dtype=torch.long) * optoi[operation]\n",
    "    second_nums = torch.randint(0, 9, (num_problems, max_digits), dtype=torch.long)\n",
    "    equals_symbols = torch.ones((num_problems,1), dtype=torch.long) * optoi['=']\n",
    "    x = torch.cat((first_nums, sum_symbols, second_nums, equals_symbols), dim=1)\n",
    "\n",
    "    # sample output data\n",
    "    masked_labels = -100 * torch.ones(num_problems, x.shape[1]-1, dtype=torch.long) # mask loss for first n inputs (-100 gets ignored by pytorch)\n",
    "    first_nums = torch.tensor([int(''.join(map(str, num.tolist()))) for num in first_nums])\n",
    "    second_nums = torch.tensor([int(''.join(map(str, num.tolist()))) for num in second_nums])\n",
    "\n",
    "    if operation == '+':\n",
    "        results = first_nums + second_nums\n",
    "    labels = [[int(digit) for digit in reversed(str(result.item()))] for result in results]\n",
    "    labels = torch.tensor(list(zip(*itertools.zip_longest(*labels, fillvalue=0))), dtype=torch.long)\n",
    "    end_tokens = torch.ones((num_problems, 1), dtype=torch.long) * optoi['<END>']\n",
    "    x = torch.cat((x, labels), dim=1)\n",
    "    y = torch.cat((masked_labels, labels, end_tokens), dim=1)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "n_samples = 1_000_000\n",
    "x, y = sample_mathproblems(n_samples)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800000, 21]),\n",
       " torch.Size([800000, 21]),\n",
       " torch.Size([100000, 21]),\n",
       " torch.Size([100000, 21]),\n",
       " torch.Size([100000, 21]),\n",
       " torch.Size([100000, 21]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * n_samples)\n",
    "val_size = int(0.9 * n_samples)\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_val, y_val = x[train_size:val_size], y[train_size:val_size]\n",
    "x_test, y_test = x[val_size:], y[val_size:]\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            if split == 'train':\n",
    "                idx = torch.randint(0, len(x_train), (batch_size, ))\n",
    "                X, Y = x_train[idx], y_train[idx]\n",
    "            elif split == 'val':\n",
    "                idx = torch.randint(0, len(x_val), (batch_size, ))\n",
    "                X, Y = x_val[idx], y_val[idx]\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_gpt(model, optimizer, train_steps=100_000, eval_iters=200):\n",
    "    for step in range(train_steps):\n",
    "        # forward pass\n",
    "        idx = torch.randint(0, len(x_train), (batch_size,))\n",
    "        x, y = x_train[idx], y_train[idx]\n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10_000 == 0:\n",
    "            losses = estimate_loss(model, eval_iters) \n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"{step}/{train_steps} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100000 - Train Loss: 2.8643, Val Loss: 2.8618\n",
      "10000/100000 - Train Loss: 0.0003, Val Loss: 0.0003\n",
      "20000/100000 - Train Loss: 0.0001, Val Loss: 0.0001\n",
      "30000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "40000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "50000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "60000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "70000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "80000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n",
      "90000/100000 - Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = GPT(16)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_gpt(model, optimizer, train_steps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 4: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([num1, op, num2, equals, real_res]), pred_res\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m x_test[:\u001b[39m10\u001b[39m]:\n\u001b[0;32m---> 13\u001b[0m     decoded_str, pred_res \u001b[39m=\u001b[39m decode(model\u001b[39m.\u001b[39;49mcalculate(example[:\u001b[39m14\u001b[39;49m]\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdecoded_str\u001b[39m}\u001b[39;00m\u001b[39m, predicted result is \u001b[39m\u001b[39m{\u001b[39;00mpred_res\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 10\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m pred_res \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, \u001b[39mreversed\u001b[39m(x[\u001b[39m-\u001b[39m(max_digits):]\u001b[39m.\u001b[39mtolist())))\n\u001b[1;32m      9\u001b[0m real_res \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(num1) \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(num2)\n\u001b[0;32m---> 10\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([num1, op, num2, equals, real_res]), pred_res\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 4: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "def decode(x):\n",
    "    x = x[0]\n",
    "    assert x[-1].item() == optoi['<END>'], \"Model did not end calculation with <END> token, result is wrong.\"\n",
    "    num1 = ''.join(map(str, x[:max_digits].tolist()))\n",
    "    op = itoop[x[max_digits].item()]\n",
    "    num2 = ''.join(map(str, x[max_digits+1:2*max_digits+1].tolist()))\n",
    "    equals = itoop[x[2*max_digits+1].item()]\n",
    "    pred_res = ''.join(map(str, reversed(x[-(max_digits):].tolist())))\n",
    "    real_res = int(num1) + int(num2)\n",
    "    return \" \".join([num1, op, num2, equals, str(real_res)]), pred_res\n",
    "\n",
    "for example in x_test[:10]:\n",
    "    decoded_str, pred_res = decode(model.calculate(example[:14].view(1, -1)))\n",
    "    print(f\"{decoded_str}, predicted result is {pred_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
