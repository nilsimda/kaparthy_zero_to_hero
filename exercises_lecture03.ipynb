{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3203)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "import string\n",
    "\n",
    "chars = sorted(set(string.ascii_lowercase))\n",
    "stoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "with open('names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "random.shuffle(names)\n",
    "n_train = int(0.8*len(names))\n",
    "n_dev = int(0.9*len(names))\n",
    "trainset = names[:n_train]\n",
    "devset = names[n_train:n_dev]\n",
    "testset = names[n_dev:]\n",
    "len(trainset), len(devset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... --> k\n",
      "...k --> h\n",
      "..kh --> y\n",
      ".khy --> r\n",
      "khyr --> i\n",
      "hyri --> e\n",
      "yrie --> .\n",
      ".... --> t\n",
      "...t --> h\n",
      "..th --> o\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_data(names, cl):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for name in names:\n",
    "        name = name + '.'\n",
    "        context = [stoi['.']]*cl\n",
    "        for ch in name:\n",
    "            ix = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "def print_context(X, y):\n",
    "    for context, label in zip(X,y):\n",
    "        con_str = ''.join(itos[ix.item()] for ix in context)\n",
    "        print(f'{con_str} --> {itos[label.item()]}')\n",
    "\n",
    "context_length = 4\n",
    "X_train, y_train = prepare_data(trainset, context_length)\n",
    "X_dev, y_dev = prepare_data(devset, context_length)\n",
    "X_test, y_test = prepare_data(testset, context_length)\n",
    "print_context(X_train[:10], y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_neuron = 200\n",
    "emb_size = 10\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "C = torch.randn(vocab_size, emb_size, generator=g, requires_grad=True)\n",
    "\n",
    "# define neural net\n",
    "W1 = torch.randn(emb_size*context_length, n_neuron, generator=g, requires_grad=True)\n",
    "b1 = torch.randn(n_neuron, generator=g, requires_grad=True)\n",
    "W2 = torch.randn(n_neuron, vocab_size, generator=g, requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2]\n",
    "\n",
    "def train_nn(X, y, batch_size=64, lr=0.1, train_steps=100_000):\n",
    "    for step in tqdm(range(train_steps)):\n",
    "        # ---forward pass---\n",
    "        ixs = torch.randint(low=0, high=X.shape[0], size=(batch_size,)) \n",
    "        mini_batch, labels = X[ixs], y[ixs]\n",
    "        embs = C[mini_batch].view(batch_size, emb_size*context_length)\n",
    "        h = torch.tanh(embs @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # ---backward pass---\n",
    "        loss.backward()\n",
    "        for param in parameters:\n",
    "            param.data += -lr * param.grad\n",
    "            param.grad = None\n",
    "    \n",
    "def eval_nn(X, y):\n",
    "    with torch.no_grad():\n",
    "        embs = C[X].view(X.shape[0], emb_size*context_length)\n",
    "        h = torch.tanh(embs @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [01:08<00:00, 4408.33it/s]\n",
      "100%|██████████| 100000/100000 [00:22<00:00, 4451.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_nn(X_train, y_train, train_steps=300_000)\n",
    "train_nn(X_train, y_train, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.061028242111206\n",
      "Dev Loss: 2.1197381019592285\n"
     ]
    }
   ],
   "source": [
    "train_loss = eval_nn(X_train, y_train)\n",
    "dev_loss = eval_nn(X_dev, y_dev)\n",
    "print(f\"Train Loss: {train_loss}\")\n",
    "print(f\"Dev Loss: {dev_loss}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outperforms the score of 2.2 on the dev set. Lets now sample some names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savan\n",
      "ikai\n",
      "yilie\n",
      "kyeorosoruarlanion\n",
      "zadi\n",
      "lindo\n",
      "izdishia\n",
      "khae\n",
      "dria\n",
      "kahdees\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "for _ in range(n_samples):\n",
    "    context = [0]*context_length\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            embs = C[torch.tensor(context)].view(1, -1)\n",
    "            h = torch.tanh(embs @ W1 + b1)\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "        print(itos[ix], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented the Mixture Model Idea from the Bengio et al. paper. I.e I use the Trigram model from the last exercise and combine it with the MLP. It achieved slightly better Devset performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2483e-05, 1.3756e-01, 3.9685e-02, 4.7487e-02, 5.3144e-02, 4.9321e-02,\n",
       "        1.2613e-02, 2.0376e-02, 2.7670e-02, 1.7684e-02, 7.4911e-02, 9.2817e-02,\n",
       "        4.9555e-02, 7.9475e-02, 3.4965e-02, 1.2105e-02, 1.6475e-02, 2.8602e-03,\n",
       "        5.2520e-02, 6.4300e-02, 4.1441e-02, 2.5871e-03, 1.1130e-02, 9.3748e-03,\n",
       "        3.9135e-03, 1.6670e-02, 2.9348e-02])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "for name in trainset:\n",
    "    chs = ['.', '.'] + list(name) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        trigram_counts[stoi[ch1], stoi[ch2], stoi[ch3]] += 1\n",
    "\n",
    "trigram_probs = (trigram_counts + 0.32).float()\n",
    "trigram_probs /= trigram_probs.sum(dim=-1, keepdim=True)\n",
    "trigram_probs[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1162450313568115"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_mixture(X, y, trigram_weight=0.5):\n",
    "    tri_prob = trigram_probs[X[:, -2], X[:, -1]]\n",
    "    with torch.no_grad():\n",
    "        embs = C[X].view(X.shape[0], emb_size*context_length)\n",
    "        h = torch.tanh(embs @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        mlp_probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    probs = trigram_weight * tri_prob + (1 - trigram_weight) * mlp_probs \n",
    "    nll= -probs[torch.arange(len(X)), y].log().mean()\n",
    "    return nll.item()\n",
    "\n",
    "eval_mixture(X_dev, y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
