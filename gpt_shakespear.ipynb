{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespear.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "encode = lambda cs: [stoi[c] for c in cs]\n",
    "decode = lambda ixs: ''.join([itos[i] for i in ixs])\n",
    "\n",
    "decode(encode(text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = torch.tensor(encode(text), dtype=torch.long)\n",
    "trainsize = int(len(encoded_text)*0.9)\n",
    "trainset = encoded_text[:trainsize]\n",
    "valset = encoded_text[trainsize:]\n",
    "trainset.shape[0], valset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([56, 53, 52, 45, 57,  1, 61, 47]),\n",
       " tensor([53, 52, 45, 57,  1, 61, 47, 58]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "batch_size = 16\n",
    "emb_dim = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = trainset if split == 'train' else valset\n",
    "    label_idx = torch.randint(context_length, len(data), (batch_size,))\n",
    "    x = torch.stack([trainset[i-context_length:i] for i in label_idx])\n",
    "    y = torch.stack([trainset[i+1-context_length:i+1] for i in label_idx])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "x[1], y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, masked=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.masked = masked\n",
    "        self.q = nn.Linear(emb_dim, emb_dim, bias=False) #compute for all heads in parrallel, n_heads * head_size = emb_dim\n",
    "        self.k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        if self.masked:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        Q = Q.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #self attention needs to be done individually for each head\n",
    "        K = K.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) #transpose needed so we do self attention in the given char context not between the different heads\n",
    "        V = V.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2)\n",
    "        wei = Q @ K.transpose(-1, -2)\n",
    "        if self.masked:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = wei / C**-0.5\n",
    "        out = F.softmax(wei, dim=-1) @ V\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "        nn.Linear(emb_dim, 4*emb_dim), nn.ReLU(),\n",
    "        nn.Linear(4*emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_head)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        out = x + self.ff(self.ln2(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads=4, n_blocks=2): \n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, emb_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_heads) for _ in range(n_blocks)])\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embs = self.token_embedding(idx) # B, T -> B, T, C\n",
    "        pos_embs = self.position_embedding(torch.arange(T))\n",
    "        out = token_embs + pos_embs\n",
    "        out = self.blocks(out)\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_tokens=10_000):\n",
    "        for _ in range(num_tokens):\n",
    "            context = idx[:, -context_length:]\n",
    "            logits, _ = self(context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_fn):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = batch_fn(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_gpt(model, optimizer, batch_fn, train_steps=100_000, eval_iters=200):\n",
    "    for step in range(train_steps):\n",
    "        # forward pass\n",
    "        x, y = batch_fn('train')\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10_000 == 0:\n",
    "            losses = estimate_loss(model, eval_iters, batch_fn) \n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"{step}/{train_steps} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000 - Train Loss: 4.4618, Val Loss: 4.4680\n"
     ]
    }
   ],
   "source": [
    "char_gpt = GPT(len(chars))\n",
    "optimizer = torch.optim.AdamW(char_gpt.parameters(), lr=1e-3)\n",
    "\n",
    "train_gpt(char_gpt, optimizer, get_batch, train_steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ange for at to the one a my cousin trio merch gough see it morn and imphiteful reaters, and where commoundr you fatherefore you new,'s drandanior puty.\n",
      "\n",
      "MEN\n",
      "Nay;\n",
      "I'll do with Byeth!\n",
      "I would not patituitty!\n",
      "\n",
      "COMINIUS:\n",
      "Tut, for it war?\n",
      "\n",
      "Five.\n",
      "\n",
      "As to was that it:\n",
      "First by in wrunked:\n",
      "Etsint there than Romeore conten it rooulive to they eat, a siter call we womarnt, Cluring let in imble durm!\n",
      "And thou was to\n",
      "\n",
      "DATLUCKINGHAM:\n",
      "A fill the marren my are mothat truate it, you:\n",
      "And,\n",
      "To you\n",
      "'t be your\n",
      "and\n",
      "Troumplours.\n",
      "O force far, the father iteor hie with me;\n",
      "And for our her sould you,\n",
      "And train. You, that tour should my to then some the hath maken belinge edowns you light me.\n",
      "\n",
      "PERCHIO:\n",
      "Be own fear'd?\n",
      "\n",
      "AUMENAMISALGUS:\n",
      "Therefate, lering humas sirest sentof Clawly you may ere done to your her a cormonts:\n",
      "If a proput thou handn?\n",
      "Theirs; his too.\n",
      "\n",
      "YONUS:\n",
      "Wherefore my her!\n",
      "\n",
      "GONUS:\n",
      "Her ment twom to outed Soed on\n",
      "mevibus!\n",
      "Prother hath her may fell'd\n",
      "So judge, not.\n",
      "\n",
      "GLOUCESTER:\n",
      "But were Humes; heat As partuoneo wert,'\n",
      "\n",
      "DUCHESTER:\n",
      "That, and not not,\n",
      "Lose be have prays Edwarva?\n",
      "For, if a arm.\n",
      "I epon thy upon to the arm I world who, and than me it her that to never\n",
      "Ista'e: shem, by her marderms to him to is fortune.\n",
      "But faitions this fall I knee, to go murness.\n",
      "\n",
      "Poor:\n",
      "Cawould wive: house or at nobapmince to-mah.\n",
      "What vice him. You art mall'd may mean, oor me toft I truiar: Wait, and in pot daught?\n",
      "My backnot to the of have your hand to-do evere what let 't!\n",
      "\n",
      "ROMEO:\n",
      "I thou woulder to hows gragpine, it this put owes wits tleaft dother\n",
      "Tare timpet drerelybling say, omnout seat,\n",
      "And herure jost:\n",
      "Name\n",
      "Of lattly love fusticolinceit her,\n",
      "And may'd,\n",
      "To that\n",
      "In a woll there thou, Our ca, or a was are of hight done,\n",
      "What in arm in na, and thou art it the lager your in to your remourn bedious;\n",
      "And lant is grasors'd so hape surs son,\n",
      "Sunt the curse; own arm is two sladom no pation me dlack in had wack I wach me.\n",
      "\n",
      "KING EDWARD:\n",
      "Wat 'cittle am, heretior, peace.\n",
      "And she sarenguitoo't!\n",
      "I why may presplict\n",
      "And Parsion---good; now hange it\n",
      "room tohe, and tabertus would were of in will with a master her;\n",
      "Leaven dobert!\n",
      "\n",
      "PULICINIUS:\n",
      "What a duking-my vitute fissious, to the come.\n",
      "I'll nam body friend, thus of Srow ex he will arain;\n",
      "There know gerther, I crownselved our do,\n",
      "Were comple\n",
      "Third my is you all.\n",
      "\n",
      "COMINIUS:\n",
      "And look\n",
      "That well in letter!\n",
      "\n",
      "GRET:\n",
      "Give you outet dening so of fevired, my word.\n",
      "She suild accusand hour grief,\n",
      "A fame of hare, have,\n",
      "I know:\n",
      "Whose unone corn fortune, and of have have it,\n",
      "That do tempal'd I flashould to iustor tentreyrief.\n",
      "Your what you?\n",
      "\n",
      "ROMEO:\n",
      "Look sayet sun\n",
      "To fortune,\n",
      "HerenUCESTENCE:\n",
      "Thou to arm his not the may for yorme us\n",
      "anoy\n",
      "is it? A see.\n",
      "\n",
      "SRRTIUS:\n",
      "If as noble,\n",
      "The she filld the more.\n",
      "\n",
      "Erest wit,--vant: come will is fell,\n",
      "his greevent it doth in a peace, one, would on manauture, my bantleman watend appard theurms, which will not by renons and kell\n",
      "Third for I speak own useit, and to her strepoeve,\n",
      "Thell your;\n",
      "And;\n",
      "pritind:\n",
      "Would Juulion it of news doet, an whus noble woult:\n",
      "Nort, makne allow that that on so is nor neunded topp'd,\n",
      "Bust, and take of merritt cord to rest.\n",
      "To devison.\n",
      "\n",
      "KING EDWARD:\n",
      "Gented out ever, there own toble blothad the or patce bird this cate her,\n",
      "Anow you mayshor bles.\n",
      "\n",
      "HERLINALUS:\n",
      "I would it cof so him.\n",
      "And, wher be hear you to then aker thet, not my flail lord. Hather, thou suiss in deleat are in Kempon. Good at turn\n",
      "The reather, and Henry couse: thou time, and think?\n",
      "\n",
      "ESCALUS:\n",
      "That dURENCE:\n",
      "Rerer, shad thou not on furtood misever HENRY VINTES:\n",
      "an all myseed;\n",
      "And in; and to sliging:\n",
      "I lord gows that Dit, our\n",
      "moover, and jud and tell,\n",
      "And world:\n",
      "Here judged you sincless; fets father art patch'd\n",
      "France The out his were the hatre it me.\n",
      "\n",
      "JOLIEN:\n",
      "Therep of Clace.\n",
      "But Roat nothou deeds, ploud troum,\n",
      "And to new to serve do slain.\n",
      "phaten and crive and eperitom,\n",
      "princlisheth is for the far; to like prettle imparest us Theirake then, may his far.\n",
      "\n",
      "GRET:\n",
      "Shou were,\n",
      "That dime and be in\n",
      "Mecter, when great our feant\n",
      "Writhes,\n",
      "I'll myseems thy prant trus!\n",
      "\n",
      "MENENIUS:\n",
      "We death, to morernst not dof Thy sorrow:\n",
      "Ay, worm her julf'd traight yet Surdom, as hath it'd;\n",
      "And feblectar,\n",
      "Aga?\n",
      "If lous be\n",
      "Imore not mand would was :\n",
      "No, I witsufleds in churward a let ad, as donger long her.\n",
      "\n",
      "MENENIUS:\n",
      "Letter\n",
      "Aut-n of our him\n",
      "As my poice with the euts the wound.\n",
      "\n",
      "ROMEO:\n",
      "Sure fond, be to a fer know and his father\n",
      "Look;\n",
      "Their that Go; Faul, and the glack is heart,\n",
      "Demn, I wis I have; my eatst reson from streption:\n",
      "Coust pointy, poses you wervillours hear my to ready camorn gray ener may for you nue, the woet or and wouns,\n",
      "Upon the lan pirot the trust exece a will fid't, not naight: when tearor ond my\n",
      "By stell brotezn Duke worm.\n",
      "\n",
      "CATES:\n",
      "Whither,\n",
      "At get to then have or of be, about other are of ime faall. I\n",
      "I death, fast,\n",
      "Than the sight ever tack it in a carrence would to woon?\n",
      "\n",
      "GLOUCESTER:\n",
      "O fier:\n",
      "And come, servaring dize lown.\n",
      "You, by well; book there' of 't have be love. Cave\n",
      "If remn him!\n",
      "And 'tis to the colding it woman?\n",
      "\n",
      "KING EL,\n",
      "Thou and harb; whichone out that it.\n",
      "\n",
      "LEOMARD:\n",
      "As and senon'd your end arsactite, make of a raroobles annot, a we world.\n",
      "\n",
      "KING EDWARD IV:\n",
      "And and plood.\n",
      "Leppitu'll it to that, whine out hive me.\n",
      "Go the prison womas; Wit, ay, and a born we me are was, both.\n",
      "\n",
      "CAFIOF, fit sit blate\n",
      "Than nevet in tirehard at is rishepht and to thy not of Epence of shreat name father,\n",
      "A the do have tase,\n",
      "Blassured sup doth 'DUCHESTER:\n",
      "Ment to recroud have you dry doou not; reraisof; shousalve, sloss that, accomentl'd, Sust ever made:\n",
      "Murdrent him, woman, heir me lotian:\n",
      "Be premn titlend were, make! Whered\n",
      "Come as lome here you man'd form!\n",
      "On mike of a was to't?\n",
      "\n",
      "KING EDBINT:\n",
      "Shourn you we make conter dever able, I prest here' neferement with a shap at rement:\n",
      "And outs to therefulll, were'erta king her, and wit fenal weet!\n",
      "First Clarence latter his past: all:\n",
      "And hand he aith setter wert\n",
      "And not cousand set,\n",
      "The gointede-reof by wear\n",
      "First not it, my lord-heakes\n",
      "The sume.\n",
      "Nown thy 'man plain.\n",
      "\n",
      "CAMILLA:\n",
      "Where's take.\n",
      "\n",
      "ROMEO:\n",
      "No body; arend it?\n",
      "\n",
      "First is place, wher\n",
      "If OF YORK:\n",
      "Butt in senestursd burn made farement:\n",
      "Nor must some jear my neatal!\n",
      "\n",
      "Put say Lord in.\n",
      "\n",
      "SICINIUS:\n",
      "Fair boinnent det Rutman:\n",
      "And upon returnius? I fayre blot.\n",
      "\n",
      "YORK:\n",
      "'Tis ward you, mad to or art I father\n",
      "Let, our maxited deshare it;'.\n",
      "\n",
      "LUCESBY:\n",
      "And tol--my omillould's Julitt this rund,\n",
      "Or what up halm!\n",
      "I thy ourtoraucuvnoul;\n",
      "An henness to you cove;\n",
      "And outs he it,\n",
      "Let scave born be Dite thou call--\n",
      "BRUCk that's deaded and of thou would mysted you of there a her they thou priness wark! where a putsourtrous me to eat, ad clat 't 'ink her, mine in him; I'lle and with a voiced I walls wast you\n",
      "way and world;\n",
      "Bits,\n",
      "And of him a laund cause,\n",
      "Wroat, the lonp gre tham o'er cet of whereasourt envercest therefore a and in thou being Glass poor that made nenbut agained inder\n",
      "My it:\n",
      "Your bout of stor, feture, seem no repone my vurthes! Wiffe, we must house:\n",
      "The ner.\n",
      "\n",
      "Fird;\n",
      "The told\n",
      "There to-gn\n",
      "Lest, awiving'd world lettlan,\n",
      "This cord is repeven, tell tusin; but to hath come to thou have tain.\n",
      "\n",
      "Firshould time,\n",
      "And vill thonour her, the stor these thy it joyder you truther and matter, my swell?\n",
      "\n",
      "LARTIUS:\n",
      "As genery!\n",
      "\n",
      "PULET:\n",
      "You\n",
      "Surpose i\n",
      "yet unfoor good were kin too my uff thou stummotion;\n",
      "Friar,\n",
      "And takness to I eem,\n",
      "He of ind.\n",
      "\n",
      "LUCINA:\n",
      "But the beyts on.\n",
      "\n",
      "MENENIUS:\n",
      "That de.'\n",
      "Merceivel new the precan dog. Nottit,\n",
      "All accotn, 'tis soundrung haped where man may's her\n",
      "and sword,\n",
      "And not; minted uffershold I shour lord, for had the majeall'd in with the impeth!\n",
      "O bad fortun to my brothink your live death, I this rove\n",
      "Of downs\n",
      "As perer,\n",
      "I'll look it.\n",
      "To shound wore.\n",
      "\n",
      "THENM: you, and make upont 'man, detter, sirth, not tear barn wherends tongunter;\n",
      "And are no I with mast.\n",
      "\n",
      "KING EDWARD:\n",
      "I have Ever I so aival but Senest.\n",
      "First him on grap as tost:\n",
      "Thereber; detidst this torthat or lie.\n",
      "Where world! this rud of the am iteor.\n",
      "\n",
      "MENENIUS:\n",
      "As could of call hers,\n",
      "My brother and parte, Notion;\n",
      "Tenters Serconter.\n",
      "Here of that.\n",
      "\n",
      "TYONGALANUS:\n",
      "I warm it.\n",
      "\n",
      "LADY LAET:\n",
      "I me as it is trean'd prief.\n",
      "\n",
      "STERMARICINO:\n",
      "But Retir to a, or\n",
      "Lame, and seen\n",
      "him, forth, that to a mound too hans:\n",
      "The arm\n",
      "The El\n",
      "Beart.\n",
      "\n",
      "HERMINALUS:\n",
      "The her, bot sun and woer so, like am ladam, vot som.\n",
      "Why,\n",
      "Han that?\n",
      "This much ourse,\n",
      "What motts servingct out misse:\n",
      "Yet trong, thered.\n",
      "\n",
      "DUKE VINCERES:\n",
      "How thou love most,\n",
      "I wall.\n",
      "Do sove wlours 'Dight whereferst to and to ve,\n",
      "And well be wit of well be blook them beine annow neeve must course.\n",
      "\n",
      "TESABELLA:\n",
      "Marcoursains:\n",
      "And well\n",
      "Alour tuch: forset do sage, or it! I have one:\n",
      "To, here to termid own and not, lord hath dome of the dount forby warrace a teature a\n",
      "und us, cove queence on to my mast mine angmen doth streperence\n",
      "I,\n",
      "Bread,\n",
      "That sing shour aliff, vale in our here's was armness and me is hight agood; hen, sir! state the aff this show; an lold book! Beard,\n",
      "What.\n",
      "\n",
      "ELBOW:\n",
      "I'll seft.\n",
      "\n",
      "GLook is guttle therely have of reless but other hor you we sust it as pilest to you volt him, are come; to mittell not to thild his eater, shood\n",
      "Sproved.\n",
      "Droy, deter some emy and wrow, craged her,\n",
      "Litter to not shem lew of Julies tark in not seat\n",
      "To werefore: her and you sport to you art is of you, badert to talk, doth nusinst.\n",
      "I, ever, it him, this ave fath to may, whereer'd,\n",
      "First to That is to red,\n",
      "Then his ex,\n",
      "Or grapless hen,\n",
      "Bevillow.\n",
      "\n",
      "CAMILLO:\n",
      "Here not.\n",
      "\n",
      "COMINI'ress?\n",
      "Conving Bast to thel jotter's curmove then not him the that en't in 'OMx Nor my to time here drace?\n",
      "\n",
      "GLAUMNAGURENCE:\n",
      "An worther, I case are doth him that retite lie, I retter I make be madious, as paboned;\n",
      "Is her! I hin king\n",
      "How botter braced that Camillour'd right awhy me I with it a carn mise\n",
      "Or art, wout aft\n",
      "I it.\n",
      "\n",
      "FAUMENCIUS:\n",
      "Ery.\n",
      "\n",
      "WARWICKNTALANUS:\n",
      "All and moust,\n",
      "And Treat laster too must: brous,\n",
      "Was as seasanders grain you\n",
      "to him say, greengther good these, to streason: tresty couould it be too,\n",
      "The So long move, long which you fearth?\n",
      "Then were a fell hare may you\n",
      "come, I'll retil, that'd me aixook, frial.\n",
      "Co\n"
     ]
    }
   ],
   "source": [
    "generated_idx = char_gpt.generate(torch.zeros((1,1), dtype=torch.long))[0].tolist()\n",
    "print(decode(generated_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 15]) torch.Size([16, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 6,  0,  3,  1, 10,  6,  2,  3,  1, 14,  2,  6,  2,  2,  1]),\n",
       " tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100,    2,    6,    2,\n",
       "            2,    1,   15]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optoi = {'+': 10, '-':11, '*':12, '/': 13, '=': 14, '<END>':15}\n",
    "itoop = {i: op for op, i in optoi.items()}\n",
    "context_length = 18\n",
    "\n",
    "# we sample two random numbers as input and their sum as the label\n",
    "def get_mathproblem(split='train'):\n",
    "    operation = '+' #TODO: Expand to all four basic operations\n",
    "    max_digits = 4 if split == 'train' else 5\n",
    "\n",
    "    # sample input data\n",
    "    first_nums = torch.randint(0, 9, (batch_size, max_digits), dtype=torch.long)\n",
    "    sum_symbols = torch.ones((batch_size,1), dtype=torch.long) * optoi[operation]\n",
    "    second_nums = torch.randint(0, 9, (batch_size, max_digits), dtype=torch.long)\n",
    "    equals_symbols = torch.ones((batch_size,1), dtype=torch.long) * optoi['=']\n",
    "    x = torch.cat((first_nums, sum_symbols, second_nums, equals_symbols), dim=1)\n",
    "\n",
    "    # sample output data\n",
    "    masked_labels = -100 * torch.ones(batch_size, x.shape[1]-1, dtype=torch.long) # mask loss for first n inputs (-100 gets ignored by pytorch)\n",
    "    first_nums = torch.tensor([int(''.join(map(str, num.tolist()))) for num in first_nums])\n",
    "    second_nums = torch.tensor([int(''.join(map(str, num.tolist()))) for num in second_nums])\n",
    "\n",
    "    if operation == '+':\n",
    "        results = first_nums + second_nums\n",
    "    labels = [[int(digit) for digit in reversed(str(result.item()))] for result in results]\n",
    "    labels = torch.tensor(list(zip(*itertools.zip_longest(*labels, fillvalue=0))), dtype=torch.long)\n",
    "    end_tokens = torch.ones((batch_size, 1), dtype=torch.long) * optoi['<END>']\n",
    "    x = torch.cat((x, labels), dim=1)\n",
    "    y = torch.cat((masked_labels, labels, end_tokens), dim=1)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = get_mathproblem('train')\n",
    "print(x.shape, y.shape)\n",
    "x[1], y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100000 - Train Loss: 2.8650, Val Loss: 3.0690\n",
      "10000/100000 - Train Loss: 0.0120, Val Loss: 17.2601\n",
      "20000/100000 - Train Loss: 0.0014, Val Loss: 15.5410\n",
      "30000/100000 - Train Loss: 0.0001, Val Loss: 17.2003\n",
      "40000/100000 - Train Loss: 0.0000, Val Loss: 16.9990\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m adder \u001b[39m=\u001b[39m GPT(\u001b[39m16\u001b[39m) \u001b[39m# numbers from 0 to 15\u001b[39;00m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(adder\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_gpt(adder, optimizer, get_mathproblem)\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mtrain_gpt\u001b[0;34m(model, optimizer, batch_fn, train_steps, eval_iters)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_steps):\n\u001b[1;32m     17\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x, y \u001b[39m=\u001b[39m batch_fn(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     21\u001b[0m     \u001b[39m# backward pass\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/general/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     15\u001b[0m pos_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(torch\u001b[39m.\u001b[39marange(T))\n\u001b[1;32m     16\u001b[0m out \u001b[39m=\u001b[39m token_embs \u001b[39m+\u001b[39m pos_embs\n\u001b[0;32m---> 17\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(out)\n\u001b[1;32m     18\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(out)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/general/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/general/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/general/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[1;32m     48\u001b[0m     out \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/general/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m wei \u001b[39m=\u001b[39m wei \u001b[39m/\u001b[39m C\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m     23\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(wei, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m@\u001b[39m V\n\u001b[0;32m---> 24\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(B, T, C)\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adder = GPT(16) # numbers from 0 to 15\n",
    "optimizer = torch.optim.AdamW(adder.parameters(), lr=1e-3)\n",
    "\n",
    "train_gpt(adder, optimizer, get_mathproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  2,  0, 15])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.tensor([1, 0, 0, 0, 10, 1, 0, 0, 0, 14], dtype=torch.long).view(1, -1)\n",
    "out = adder.generate(example, num_tokens=6)\n",
    "out[0, 10:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  4,  2,  4,  0, 15])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
